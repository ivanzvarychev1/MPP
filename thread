from queue import Queue, Empty # импортируем библиотеку очереди
from threading import Thread # ипортируем библиотеку для многопоточного программирования
from bs4 import BeautifulSoup as BS #Библиотека для парсинга страницы 
from requests import get   #импортируем метод get для запросов
from pymorphy2 import MorphAnalyzer as MA #морфологический анализатор для русского языка
from nltk.tokenize import PunktSentenceTokenizer as PST  #Класс для выделения предложений
from nltk.tokenize import WordPunctTokenizer as WPT      #класс для разделения слов в предложении

ma = MA()       #обозначение переменных как класс
st = PST()	#обозначение переменных как класс
wt = WPT()	#обозначение переменных как класс

Geoxx = {"city": {}}

q = Queue() # создаем переменную q типа очередь

try:  #попытаться
	my_file = open("some.txt", "r", encoding='utf-8')   #открыть файл с именем some.txt с кодировкой utf-8 на чтение
	text = my_file.read()	#в переменную text запоминаем все данные из этого файла
except FileNotFoundError:	#если файл не найден отрабатываем исключение
	text = BS(get("http://lib.ru/KRAPIWIN/airplane.txt").content, fromEncoding="windows-1251").text	# парсим страницу
	# и заносим в переменную text
	my_file = open("some.txt", "w", encoding='utf-8')	#создаем файл на запись с кодировкой utf-8	
	my_file.write(text)	#записываем в файл данные, которые спарсили
	
text_=st.sentences_from_text(text) # заносим в переменную text_ список разбитый по предложениям
lenS=len(text_) # заносим в переменную lenS количество предложений из text_

def sentence_paral(): #функция
	_Kx = q.get() # Достаем данные из очереди
	for k in _Kx: # бежим по этим данным
		xGeoxx = text_[k] # в xNames заносим предложение k
		for word in wt.tokenize(xGeoxx): #бежим по словам в этом предложении
			for p in ma.parse(word): #морфологический разбор слова
				if "Geox" in p.tag and p.score>=0.4:#если мы считаем, что вероятность больше 0.4 и слово = имя
									# то мы считаем это слово именем
					if Geoxx["city"].get(p.normal_form) is None:
						Geoxx["city"].update({p.normal_form:1})
					else:
						Geoxx["city"][p.normal_form]+=1
							
N = 4 # задаем количество процессов
d = int(lenS/N) # считаем количество предложений разбитых равно на 4 процесса 
n,m = 0,d # создаем промежутки из этих предложений
T = [] # пустой список

for f in range(0,N): #цикл по N
	test=range(n,m) # заносим в test промежуток из значеий от n до m
	q.put(test) # заносим значения из test в очередь q
	t = Thread(target = sentence_paral) #выбираем цель threding'a - функцию
	t.start() # запускаем метод start()
	n+=d # увеличиваем промежутки
	m+=d # увеличиваем промежутки
	T+=[t] # заносим в список t

for t in T:
	t.join() # вызываем метод join

print(Geoxx) # Вывод имен
